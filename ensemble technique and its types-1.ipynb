{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7391adee-ec53-474b-acdf-0972380bb47f",
   "metadata": {},
   "source": [
    "# Question.1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "939dab57-cb15-42bc-8f1c-316d506d26ea",
   "metadata": {},
   "source": [
    "## What is an ensemble technique in machine learning?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "194512cf-b473-4f0a-83c4-e4b8a29c214f",
   "metadata": {},
   "source": [
    "An ensemble technique in machine learning refers to the strategy of combining multiple individual models (often called \"base models\" or \"learners\") to create a stronger, more accurate, and more robust model. The idea behind ensemble methods is to leverage the diversity and collective wisdom of multiple models to improve overall predictive performance and reduce the risk of individual model errors.\n",
    "\n",
    "Ensemble techniques are particularly effective when individual models have complementary strengths and weaknesses or when different models capture different aspects of the underlying data patterns. By aggregating the predictions of these individual models, ensemble methods aim to achieve better generalization and minimize the impact of model biases and overfitting.\n",
    "\n",
    "There are several types of ensemble techniques in machine learning, including:\n",
    "\n",
    "1. **Bagging (Bootstrap Aggregating)**:\n",
    "   Bagging involves training multiple instances of the same model on different subsets of the training data, often using bootstrapping. These models make independent predictions, and their outputs are combined, typically through averaging (for regression) or majority voting (for classification).\n",
    "\n",
    "2. **Boosting**:\n",
    "   Boosting focuses on sequentially training multiple models, where each new model is designed to correct the errors made by the previous models. The predictions of these models are combined, with more weight given to models that perform well on difficult cases.\n",
    "\n",
    "3. **Random Forest**:\n",
    "   Random Forest is an ensemble technique that combines the concepts of bagging and random feature selection. It creates a forest of decision trees, each trained on different subsets of data and features, and aggregates their predictions.\n",
    "\n",
    "4. **Stacking**:\n",
    "   Stacking involves training multiple diverse models and using their predictions as input features for a final \"meta-model\" that makes the ultimate prediction. Stacking can capture complex relationships between models and data.\n",
    "\n",
    "5. **Voting (Majority/Plurality Voting)**:\n",
    "   Voting combines the predictions of multiple models by taking the majority vote (for classification) or the average (for regression) of the predictions. It's commonly used in ensemble methods to make a final decision.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dd8f3b6-4f69-4e99-aa89-a3028c769af3",
   "metadata": {},
   "source": [
    "# Question.2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba58f9ea-477c-46cd-8b3b-9326450433f5",
   "metadata": {},
   "source": [
    "## Why are ensemble techniques used in machine learning?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f572fec-adf3-43c0-bf19-5f710341ed88",
   "metadata": {},
   "source": [
    "Ensemble techniques are used in machine learning for several compelling reasons, as they offer a range of benefits that can significantly enhance the performance, robustness, and reliability of predictive models. Here are some key reasons why ensemble techniques are widely employed:\n",
    "\n",
    "1. **Improved Predictive Performance**:\n",
    "   Ensemble methods often yield better predictive performance compared to individual models. By combining the strengths of multiple models, ensembles can capture a broader range of data patterns and relationships, leading to more accurate predictions.\n",
    "\n",
    "2. **Reduction of Overfitting**:\n",
    "   Ensemble techniques help mitigate the risk of overfitting by combining predictions from multiple models that may have different sources of error. This reduction in overfitting is particularly valuable when working with complex data or limited training samples.\n",
    "\n",
    "3. **Robustness to Noise and Outliers**:\n",
    "   Ensembles are less sensitive to noise and outliers in the data due to their ability to average out errors and identify more stable patterns in the aggregate predictions.\n",
    "\n",
    "4. **Handling Model Biases**:\n",
    "   Different models may have inherent biases or assumptions. Ensemble methods can mitigate these biases by incorporating predictions from various models, leading to a more balanced and accurate overall prediction.\n",
    "\n",
    "5. **Capture of Diverse Patterns**:\n",
    "   Ensembles can capture a wider variety of underlying patterns present in the data. This is especially useful when individual models perform well on different subsets of the data or capture distinct aspects of the problem.\n",
    "\n",
    "6. **Increased Generalization**:\n",
    "   Ensemble methods can lead to improved generalization to new, unseen data by reducing the impact of model-specific errors and biases. This results in more reliable and consistent predictions.\n",
    "\n",
    "7. **Versatility Across Algorithms**:\n",
    "   Ensemble techniques can be applied to various types of base models, including decision trees, linear models, support vector machines, and more. This flexibility allows for adaptation to different problem domains.\n",
    "\n",
    "8. **Handling Model Complexity**:\n",
    "   Ensembles can manage complex relationships in the data without relying solely on a single complex model, making them suitable for problems where the underlying patterns are intricate.\n",
    "\n",
    "9. **Interpretability and Transparency**:\n",
    "   Some ensemble techniques, like bagging and random forests, provide feature importance measures that help understand which features contribute most to predictions. This can aid in model interpretability.\n",
    "\n",
    "10. **Proven Effectiveness**:\n",
    "    Ensembles have been shown to win numerous machine learning competitions and benchmarks. They are considered state-of-the-art techniques for a wide range of tasks.\n",
    "\n",
    "11. **Model Combination**:\n",
    "    Ensembles can combine multiple models that specialize in different aspects of the problem, creating a more holistic solution that leverages the strengths of each individual model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba81c97f-f1ed-4710-8305-f6b0d926bc43",
   "metadata": {},
   "source": [
    "# Question.3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b01f9b04-f702-490a-850f-b782916073f3",
   "metadata": {},
   "source": [
    "## What is bagging?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08287a91-d0b3-49bc-8180-6cd9c217390f",
   "metadata": {},
   "source": [
    "Bagging, short for Bootstrap Aggregating, is an ensemble learning technique used to improve the performance and generalization of machine learning models, particularly decision trees. Bagging involves training multiple instances of the same model on different subsets of the training data, followed by aggregating their predictions to make a final prediction. This technique helps reduce overfitting and increase the model's robustness.\n",
    "\n",
    "Here's how bagging works:\n",
    "\n",
    "1. **Bootstrapping**:\n",
    "   The process begins by creating multiple random subsets of the training data, each of the same size as the original dataset. This is done by sampling with replacement from the original data, which means that some data points might appear multiple times in a subset, while others might not appear at all.\n",
    "\n",
    "2. **Model Training**:\n",
    "   For each subset of the data, an individual model is trained using the same learning algorithm. The subsets allow each model to be exposed to different variations of the training data.\n",
    "\n",
    "3. **Independent Predictions**:\n",
    "   Each individual model makes predictions independently for new, unseen data points based on its training subset.\n",
    "\n",
    "4. **Aggregation of Predictions**:\n",
    "   The final prediction of the bagging ensemble is obtained by aggregating the predictions made by all the individual models. For classification tasks, this typically involves using majority voting to select the most frequent predicted class label. For regression tasks, the predictions are averaged.\n",
    "\n",
    "5. **Reducing Variance**:\n",
    "   One of the main benefits of bagging is its ability to reduce the variance of the model's predictions. By training models on different subsets of data, the models tend to have different sources of error. Aggregating their predictions helps to \"smooth out\" these errors and create a more stable and accurate prediction.\n",
    "\n",
    "6. **Generalization Improvement**:\n",
    "   Bagging improves generalization by creating an ensemble of models that perform well on different parts of the data. This ensemble approach reduces the model's reliance on any one particular subset of data and helps the model generalize better to unseen data.\n",
    "\n",
    "7. **Algorithm Compatibility**:\n",
    "   Bagging can be applied to a variety of base models, including decision trees, random forests, support vector machines, and more."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cdcc2ac-8a90-4930-b416-05440d218a5d",
   "metadata": {},
   "source": [
    "# Question.4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b25abd0d-399c-4181-b0c2-9ffc543dbc6a",
   "metadata": {},
   "source": [
    "## What is boosting?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de51a61d-f68a-48a9-b5e4-dde3bdb4ae09",
   "metadata": {},
   "source": [
    "Boosting is an ensemble learning technique that aims to improve the performance of machine learning models by sequentially training a series of weak models and combining their predictions to create a strong model. Unlike bagging, which focuses on reducing variance, boosting focuses on reducing bias and improving accuracy by focusing on correcting errors made by previous models.\n",
    "\n",
    "Here's how boosting works:\n",
    "\n",
    "1. **Sequential Model Training**:\n",
    "   Boosting involves training a series of weak models (also called \"base learners\" or \"weak learners\") sequentially. A weak model is a model that performs slightly better than random chance but is not necessarily very accurate on its own.\n",
    "\n",
    "2. **Iterative Process**:\n",
    "   The training process is iterative. In each iteration, a new weak model is trained using the training data. The focus is on capturing the patterns that were missed by previous models.\n",
    "\n",
    "3. **Adaptive Data Weighting**:\n",
    "   Boosting assigns weights to each data point in the training set. Initially, all data points are assigned equal weights. However, in subsequent iterations, the weights of misclassified data points are increased, making them more influential in training the next model. This adaptive weighting gives more attention to difficult cases.\n",
    "\n",
    "4. **Error Correction**:\n",
    "   Each new model is trained to correct the errors made by the previous models. The idea is to focus on the examples that are misclassified or have high residual errors.\n",
    "\n",
    "5. **Combining Predictions**:\n",
    "   The final prediction of the boosting ensemble is obtained by combining the predictions of all the individual models. However, unlike simple averaging, each model's prediction is weighted based on its performance on the training data.\n",
    "\n",
    "6. **Bias Reduction**:\n",
    "   Boosting is particularly effective in reducing bias and improving model accuracy. By focusing on challenging cases and emphasizing their importance, the ensemble of models becomes progressively better at capturing complex patterns.\n",
    "\n",
    "7. **Weak to Strong**:\n",
    "   While each individual model might be weak, the combination of these models in boosting results in a strong, accurate model. This is achieved through the iterative error correction process.\n",
    "\n",
    "8. **Potential for Overfitting**:\n",
    "   While boosting aims to reduce bias, it can increase variance if the weak models start overfitting to the training data. Techniques like early stopping can help prevent this."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "432f5473-a17d-4124-986a-b0b340787677",
   "metadata": {},
   "source": [
    "# Question.5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a69c9fb6-16ba-4b8a-a61f-60f2fed3d46c",
   "metadata": {},
   "source": [
    "## What are the benefits of using ensemble techniques?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efee33a4-8a3c-4eea-b232-79252428724a",
   "metadata": {},
   "source": [
    "Ensemble techniques are machine learning methods that combine the predictions of multiple models to improve overall predictive performance. These techniques offer several benefits that make them popular and effective in various applications:\n",
    "\n",
    "1. **Improved Accuracy and Generalization:** Ensembling combines the strengths of multiple models, compensating for individual model weaknesses and errors. It can lead to higher accuracy and better generalization to new, unseen data.\n",
    "\n",
    "2. **Reduced Overfitting:** Ensembles can reduce overfitting, as combining multiple models helps to mitigate the tendency of individual models to fit noise in the training data.\n",
    "\n",
    "3. **Stability and Robustness:** Ensembles are more robust to variations in the training data and are less likely to be influenced by outliers or noisy data points. This can lead to more stable and reliable predictions.\n",
    "\n",
    "4. **Handling Complex Relationships:** Different models might capture different aspects of complex relationships in the data. Ensembles allow these diverse perspectives to be combined, leading to a more comprehensive understanding of the underlying patterns.\n",
    "\n",
    "5. **Model Selection Simplification:** Instead of spending significant effort on hyperparameter tuning for a single model, ensembles can help capture a broader range of hyperparameter settings, reducing the risk of selecting suboptimal configurations.\n",
    "\n",
    "6. **Flexibility:** Ensembling is not restricted to a specific type of base model. It can combine different types of algorithms, such as decision trees, neural networks, and support vector machines, allowing you to leverage the strengths of each.\n",
    "\n",
    "7. **Enabling Parallelism:** Many ensemble algorithms can be trained in parallel, leading to reduced training times on multi-core systems or distributed computing platforms.\n",
    "\n",
    "8. **Interpretability:** In some cases, ensemble methods can provide insights into the importance of features or relationships in the data by analyzing how different models contribute to the final prediction.\n",
    "\n",
    "Common ensemble techniques include:\n",
    "\n",
    "- **Bagging (Bootstrap Aggregating):** This involves training multiple instances of the same model on different subsets of the training data (bootstrapped samples) and averaging their predictions. Random Forest is a popular example of a bagging-based ensemble.\n",
    "\n",
    "- **Boosting:** Boosting involves training a sequence of weak learners (models that perform slightly better than random chance) in a weighted manner, where each subsequent model focuses on the errors made by the previous ones. AdaBoost and Gradient Boosting Machines (GBM) are popular boosting algorithms.\n",
    "\n",
    "- **Stacking:** Stacking combines the predictions of multiple models through a meta-model, which learns to combine the base models' outputs. It can involve multiple layers of models, with lower layers making predictions and higher layers combining those predictions.\n",
    "\n",
    "- **Voting:** In this technique, different models make predictions on the same data, and the final prediction is determined by a majority vote or weighted vote.\n",
    "\n",
    "Ensemble techniques, however, are not a one-size-fits-all solution. They can be computationally expensive and might not always lead to improvements, especially if the individual models are highly correlated or if the data is inherently noisy. It's important to carefully choose and tune the ensemble method based on the problem at hand and the characteristics of the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81222990-ae90-4edb-b428-e5c007c561e3",
   "metadata": {},
   "source": [
    "# Question.6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "573bb7d3-b248-4033-a14b-e8c77e401947",
   "metadata": {},
   "source": [
    "## Are ensemble techniques always better than individual models?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ceb776a-eb54-4bc6-a04e-25eaece8b9fa",
   "metadata": {},
   "source": [
    "Ensemble techniques are not guaranteed to always outperform individual models. While they offer significant benefits in many scenarios, there are situations in which using ensemble methods might not provide better results:\n",
    "\n",
    "1. **Limited Data:** Ensembles require multiple models to be trained, which can be computationally expensive and may not be feasible when you have limited training data.\n",
    "\n",
    "2. **Simple Data:** If the data is relatively simple and the relationships can be captured effectively by a single model, using an ensemble might not provide substantial improvements and could potentially introduce unnecessary complexity.\n",
    "\n",
    "3. **High Computational Cost:** Ensembles involve training multiple models, which can be resource-intensive in terms of computation and memory. In situations where resources are limited, training and deploying an ensemble might not be practical.\n",
    "\n",
    "4. **Correlated Models:** If the base models in an ensemble are highly correlated (i.e., they make similar errors), the ensemble might not provide significant improvement. Diversity among base models is a key factor in the success of ensembles.\n",
    "\n",
    "5. **Overfitting:** Ensembles can reduce overfitting, but if not managed properly, they can still overfit the training data. Careful selection of base models, hyperparameters, and validation strategies is essential.\n",
    "\n",
    "6. **Interpretability:** Ensembles are often more complex than individual models, which can make them harder to interpret and explain. If interpretability is a critical requirement, a simpler individual model might be preferred.\n",
    "\n",
    "7. **Implementation Complexity:** Implementing and maintaining ensemble methods can be more complex compared to working with a single model. It might require additional effort in terms of code, testing, and deployment.\n",
    "\n",
    "8. **Domain-Specific Knowledge:** In some cases, domain-specific knowledge might indicate that certain models or approaches are more appropriate. Ensembles might not always align with the specific characteristics of the problem domain.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0e44ad8-5f13-4bab-a69e-9dc24bcfe5d3",
   "metadata": {},
   "source": [
    "# Question.7"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "342beca9-2a7c-4f6c-8f51-9883aefb20ff",
   "metadata": {},
   "source": [
    "## How is the confidence interval calculated using bootstrap?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51dce943-693c-493e-beac-8fb2785f6551",
   "metadata": {},
   "source": [
    "The bootstrap method is a resampling technique used to estimate the sampling distribution of a statistic by repeatedly resampling with replacement from the original data. It can also be used to calculate confidence intervals for various statistics, including means, medians, standard deviations, and more. The basic idea behind bootstrap confidence intervals is to repeatedly sample from the data to simulate different datasets and then calculate the statistic of interest for each simulated dataset. The distribution of these statistics is used to estimate the confidence interval.\n",
    "\n",
    "Here's a general process for calculating a bootstrap confidence interval:\n",
    "\n",
    "1. **Data Collection:** Start with your original dataset of size \\(n\\).\n",
    "\n",
    "2. **Resampling:** Repeatedly draw random samples (with replacement) from the original dataset to create a new dataset of the same size (\\(n\\)). Each new dataset is called a \"bootstrap sample.\"\n",
    "\n",
    "3. **Statistic Calculation:** Calculate the desired statistic (e.g., mean, median, standard deviation) for each bootstrap sample.\n",
    "\n",
    "4. **Creating the Confidence Interval:** Sort the calculated statistics from step 3. To create a confidence interval, you can select the desired percentage of the sorted statistics as the lower and upper bounds of the interval. For example, for a 95% confidence interval, you would select the 2.5th percentile as the lower bound and the 97.5th percentile as the upper bound.\n",
    "\n",
    "5. **Interpreting the Confidence Interval:** The resulting confidence interval represents the range of values within which the true population parameter (e.g., mean, median) is likely to fall with the specified level of confidence (e.g., 95%).\n",
    "\n",
    "Here's a basic example using the mean as the statistic of interest:\n",
    "\n",
    "1. Collect your original dataset of size \\(n\\).\n",
    "\n",
    "2. Create a large number of bootstrap samples (e.g., 1000) by randomly selecting \\(n\\) data points (with replacement) from the original dataset for each sample.\n",
    "\n",
    "3. Calculate the mean for each bootstrap sample.\n",
    "\n",
    "4. Sort the calculated means.\n",
    "\n",
    "5. Choose the desired confidence level (e.g., 95%) and find the corresponding percentiles of the sorted means. For a 95% confidence interval, you would select the mean at the 2.5th percentile as the lower bound and the mean at the 97.5th percentile as the upper bound.\n",
    "\n",
    "6. The resulting interval represents the estimated confidence interval for the population mean.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab4fa323-d531-4377-8f29-dd801c293caa",
   "metadata": {},
   "source": [
    "# Question.8"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23b71fa2-8566-45c7-a200-d39be627bcba",
   "metadata": {},
   "source": [
    "## How does bootstrap work and What are the steps involved in bootstrap?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fec0c75-eff3-4610-be4e-35a3450c54ca",
   "metadata": {},
   "source": [
    "Bootstrap is a resampling technique used for statistical inference. It's particularly useful when you want to make inferences about the population based on a sample, without making strong distributional assumptions. Bootstrap allows you to estimate the sampling distribution of a statistic by repeatedly resampling from the observed data.\n",
    "\n",
    "Here are the steps involved in the bootstrap procedure:\n",
    "\n",
    "1. **Data Collection:** Start with your original dataset of size \\(n\\), which represents your sample.\n",
    "\n",
    "2. **Resampling with Replacement:** Generate many (often thousands) of bootstrap samples by randomly selecting data points from the original dataset with replacement. This means that each bootstrap sample can have repeated instances of the same data point, and some data points might be left out.\n",
    "\n",
    "3. **Statistical Calculation:** Calculate the desired statistic of interest (e.g., mean, median, standard deviation, etc.) for each of the bootstrap samples. This gives you a collection of statistics that represent how the statistic would vary across different samples drawn from the original dataset.\n",
    "\n",
    "4. **Analyzing the Distribution:** With the collection of statistics obtained from step 3, you can analyze the distribution of these statistics. This distribution is called the \"bootstrap distribution.\" It provides insights into the variability of the statistic of interest when calculated from different samples.\n",
    "\n",
    "5. **Confidence Intervals:** To construct a confidence interval for the population parameter (e.g., mean), you sort the bootstrap statistics and find percentiles to define the interval. For a 95% confidence interval, you would typically use the 2.5th and 97.5th percentiles.\n",
    "\n",
    "6. **Inference:** You can use the bootstrap distribution to make inferences about the population parameter. For instance, you can make statements like \"We are 95% confident that the true population mean lies within this interval.\"\n",
    "\n",
    "Key Points to Remember:\n",
    "\n",
    "- Bootstrap essentially mimics the process of drawing multiple samples from the same dataset to estimate the sampling variability.\n",
    "- It's important to generate a large number of bootstrap samples to obtain a reliable estimate of the sampling distribution.\n",
    "- Bootstrap is particularly useful when you have limited data or when making assumptions about the distribution of the data is challenging.\n",
    "- While bootstrap can provide valuable insights, it doesn't replace the need for careful statistical analysis and domain knowledge.\n",
    "- Bootstrap is not appropriate for all types of data and situations, and understanding its assumptions and limitations is crucial."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "797233cb-5275-4200-9014-088b58d90ae8",
   "metadata": {},
   "source": [
    "# Question.9"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52c23cbc-4749-44cf-84ff-f890c9592180",
   "metadata": {},
   "source": [
    "## A researcher wants to estimate the mean height of a population of trees. They measure the height of a sample of 50 trees and obtain a mean height of 15 meters and a standard deviation of 2 meters. Use bootstrap to estimate the 95% confidence interval for the population mean height."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bc2bb0c1-df5b-4d1a-a8e5-bdb8bbee30b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "95% Confidence Interval: [14.45803946 15.59346987]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "sample_mean = 15\n",
    "sample_std = 2\n",
    "sample_size = 50\n",
    "num_bootstrap_samples = 1000\n",
    "bootstrap_means = []\n",
    "for _ in range(num_bootstrap_samples):\n",
    "    bootstrap_sample = np.random.normal(sample_mean, sample_std, sample_size)\n",
    "    bootstrap_mean = np.mean(bootstrap_sample)\n",
    "    bootstrap_means.append(bootstrap_mean)\n",
    "confidence_interval = np.percentile(bootstrap_means, [2.5, 97.5])\n",
    "print(\"95% Confidence Interval:\", confidence_interval)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "343c073e-8796-45a0-8eb8-43eb6fb178bc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4ec1718-32e2-4207-ab1c-4b3ebcbc5481",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
