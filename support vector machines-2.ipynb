{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "511432aa-6dbb-492a-98bd-851f4d613be6",
   "metadata": {},
   "source": [
    "# Question.1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72f2b4b1-fee4-4eff-a524-41e04250bef7",
   "metadata": {},
   "source": [
    "## What is the relationship between polynomial functions and kernel functions in machine learning algorithms?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f6126a3-d320-4454-8dc4-daac1652c156",
   "metadata": {},
   "source": [
    "Polynomial functions and kernel functions are both concepts used in machine learning algorithms, particularly in the context of support vector machines (SVMs) and kernel methods. Let's delve into each of them and explore their relationship:\n",
    "\n",
    "1. **Polynomial Functions:**\n",
    "A polynomial function is a mathematical expression that consists of one or more terms, each containing a variable raised to a non-negative integer exponent, multiplied by a coefficient. The general form of a polynomial of degree \"d\" is: \n",
    "\n",
    "\\[ f(x) = a_d x^d + a_{d-1} x^{d-1} + \\ldots + a_1 x + a_0 \\]\n",
    "\n",
    "Polynomial functions can capture a wide range of shapes and patterns, making them versatile for modeling various relationships between input features and output values.\n",
    "\n",
    "2. **Kernel Functions:**\n",
    "Kernel functions are central to the concept of kernel methods, which are used in various machine learning algorithms, particularly in SVMs. In the context of SVMs, a kernel function measures the similarity between two data points in a transformed feature space. The idea behind kernel methods is to implicitly map the input data into a higher-dimensional space where it might be easier to separate classes with a hyperplane. However, instead of explicitly computing this mapping, which could be computationally expensive, kernel functions allow us to calculate the inner product of the mapped points directly in the original input space.\n",
    "\n",
    "Common kernel functions include linear, polynomial, radial basis function (RBF), and sigmoid kernels. Of interest here is the polynomial kernel.\n",
    "\n",
    "3. **Relationship:**\n",
    "The relationship between polynomial functions and kernel functions comes into play through the polynomial kernel in the context of SVMs. The polynomial kernel is a specific type of kernel function that computes the inner product of data points after applying a polynomial transformation. The formula for the polynomial kernel of degree \"d\" is:\n",
    "\n",
    "\\[ K(x, y) = (x^T y + c)^d \\]\n",
    "\n",
    "Here, \\(x\\) and \\(y\\) are data points, \\(c\\) is a constant term, and \\(d\\) is the degree of the polynomial.\n",
    "\n",
    "The polynomial kernel essentially allows us to implicitly use a polynomial transformation on the input data while working in the original input space. This means that the SVM, using the polynomial kernel, can learn decision boundaries that are effectively polynomial functions of the original input features. In other words, the polynomial kernel provides a way to capture non-linear relationships between data points without explicitly computing the transformed feature vectors.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5e809bc-ae44-42f4-ac59-ee37f1448b34",
   "metadata": {},
   "source": [
    "# Question.2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4a5e7cf-0041-458c-bc0b-a913358c7904",
   "metadata": {},
   "source": [
    "## How can we implement an SVM with a polynomial kernel in Python using Scikit-learn?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a1d1f5f-8d2d-42db-b0f7-5470e98fd31a",
   "metadata": {},
   "source": [
    "Implementing an SVM with a polynomial kernel in Python using Scikit-learn is quite straightforward. Scikit-learn provides a well-designed API for building and training machine learning models, including SVMs. Here's a step-by-step guide on how to implement an SVM with a polynomial kernel using Scikit-learn:\n",
    "\n",
    "1. **Import Necessary Libraries:**\n",
    "   Start by importing the required libraries.\n",
    "\n",
    "```python\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "```\n",
    "\n",
    "2. **Load and Prepare Data:**\n",
    "   Load a dataset and split it into training and testing sets.\n",
    "\n",
    "```python\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "```\n",
    "\n",
    "3. **Create and Train SVM with Polynomial Kernel:**\n",
    "   Create an SVM model with a polynomial kernel and train it on the training data.\n",
    "\n",
    "```python\n",
    "degree = 3  \n",
    "C = 1.0   \n",
    "svm_poly = SVC(kernel='poly', degree=degree, C=C)\n",
    "svm_poly.fit(X_train, y_train)\n",
    "```\n",
    "\n",
    "4. **Make Predictions and Evaluate:**\n",
    "   Use the trained SVM model to make predictions on the test data and evaluate its performance.\n",
    "\n",
    "```python\n",
    "y_pred = svm_poly.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n",
    "```\n",
    "\n",
    "That's it! You've successfully implemented an SVM with a polynomial kernel using Scikit-learn. Just modify the dataset loading and the hyperparameters (such as `degree` and `C`) as needed for your specific problem.\n",
    "\n",
    "Remember that the degree of the polynomial kernel controls the complexity of the model. Higher degrees can lead to overfitting, so it's important to tune this hyperparameter based on cross-validation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bee7507-4bb2-4e36-b02d-04f8aa82d3ea",
   "metadata": {},
   "source": [
    "# Question.3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93817496-8ebf-4e4e-925e-85abd04fbce4",
   "metadata": {},
   "source": [
    "## How does increasing the value of epsilon affect the number of support vectors in SVR?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddbae2ff-84f5-4874-be51-ae1617f50f73",
   "metadata": {},
   "source": [
    "In Support Vector Regression (SVR), epsilon (\\(\\epsilon\\)) is a hyperparameter that controls the width of the epsilon-insensitive tube around the regression line. This tube defines a region within which errors are considered negligible and not penalized. Data points falling within this tube are not considered support vectors, even if they're inside the margin. Points that fall outside the tube contribute to the formation of the support vectors.\n",
    "\n",
    "In SVR, support vectors are the data points that lie on the margin or within the margin and contribute to defining the regression line. They are the points that are most influential in determining the final regression model. The number of support vectors can significantly impact the model's complexity, training time, and generalization performance.\n",
    "\n",
    "The relationship between the value of epsilon and the number of support vectors in SVR can be understood as follows:\n",
    "\n",
    "1. **Smaller Epsilon:**\n",
    "   When the value of epsilon is small, the epsilon-insensitive tube is narrow, and only data points very close to the regression line are considered negligible in terms of error. This means that a larger number of data points could fall outside the tube, leading to more points being classified as support vectors. The SVR model is likely to be more sensitive to individual data points, and it may capture finer variations in the data. As a result, the model's complexity could increase, potentially leading to overfitting.\n",
    "\n",
    "2. **Larger Epsilon:**\n",
    "   When the value of epsilon is large, the epsilon-insensitive tube is wider, and more data points are considered negligible in terms of error. This could lead to fewer points falling outside the tube, resulting in fewer support vectors. The model would be less influenced by individual data points, and it might capture more general trends in the data. The model's complexity could be lower, which might help prevent overfitting.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "939320d3-3f17-4049-8d3b-cbaaf2bd005e",
   "metadata": {},
   "source": [
    "# Question.4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89d7519c-3135-496b-ba4e-dd123f1522d8",
   "metadata": {},
   "source": [
    "## How does the choice of kernel function, C parameter, epsilon parameter, and gamma parameter affect the performance of Support Vector Regression (SVR)? Can you explain how each"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8796957c-aa3d-48ff-8803-2576b6b0ba06",
   "metadata": {},
   "source": [
    "\n",
    "**1. Kernel Function:**\n",
    "Kernel functions determine how the SVR algorithm maps the input data into a higher-dimensional space where it can find a linear relationship. Different kernel functions are suitable for different types of data distributions. Common kernel functions include linear, polynomial, radial basis function (RBF), and sigmoid kernels.\n",
    "\n",
    "- **Linear Kernel:** Suitable for linear relationships in the data. Best when the data is already linear or close to linear.\n",
    "- **Polynomial Kernel:** Useful for capturing polynomial relationships. The degree of the polynomial is a key parameter.\n",
    "- **RBF Kernel:** Effective for capturing complex, non-linear relationships. The gamma parameter is important.\n",
    "- **Sigmoid Kernel:** Can be used to model non-linear relationships, but may not perform as well as RBF for complex data.\n",
    "\n",
    "**2. C Parameter:**\n",
    "The C parameter controls the trade-off between minimizing the training error and maximizing the margin (soft margin) in SVR. It influences the model's sensitivity to errors and overfitting.\n",
    "\n",
    "- Smaller C: The model is more tolerant of errors, allowing more points to fall outside the margin. This may lead to a larger number of support vectors, resulting in a more complex model.\n",
    "- Larger C: The model is less tolerant of errors, prioritizing a smaller margin. This could result in fewer support vectors and potentially better generalization, but it might also lead to overfitting if set too high.\n",
    "\n",
    "**3. Epsilon Parameter:**\n",
    "The epsilon parameter defines the width of the epsilon-insensitive tube around the regression line. It controls the tolerance for errors within this tube.\n",
    "\n",
    "- Smaller Epsilon: The tube is narrow, making the model sensitive to errors and possibly leading to overfitting.\n",
    "- Larger Epsilon: The tube is wider, making the model less sensitive to errors and encouraging a simpler model.\n",
    "\n",
    "**4. Gamma Parameter:**\n",
    "The gamma parameter is specific to the RBF kernel. It controls the influence of a single training example and how far its influence reaches.\n",
    "\n",
    "- Smaller Gamma: Wider influence, resulting in a smoother decision boundary and potentially better generalization.\n",
    "- Larger Gamma: Narrower influence, causing the decision boundary to be more sensitive to individual data points and possibly leading to overfitting.\n",
    "\n",
    "In summary:\n",
    "\n",
    "- **Kernel Function:** Choose the kernel based on the data's underlying relationship (linear, polynomial, non-linear, etc.).\n",
    "- **C Parameter:** Adjust to control the trade-off between error tolerance and model complexity.\n",
    "- **Epsilon Parameter:** Adjust to control the width of the epsilon-insensitive tube.\n",
    "- **Gamma Parameter:** Relevant for RBF kernel. Adjust to control the influence of individual training examples.\n",
    "\n",
    "Finding the right combination of these parameters is often a process of experimentation and hyperparameter tuning. Techniques like grid search and cross-validation can help identify the best set of parameters for your specific problem. Keep in mind that the impact of these parameters can depend on the dataset, the nature of the problem, and the desired trade-offs between model complexity and performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ca3ec25-dc43-49af-97e4-dc7f5996be43",
   "metadata": {},
   "source": [
    "# Question.5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d012c4ab-32fe-4b07-b948-ab1baa429af5",
   "metadata": {},
   "source": [
    "## Assignment:\n",
    "* Import the necessary libraries and load the dataset\n",
    "* Split the dataset into training and testing sets\n",
    "* Preprocess the data using any technique of your choice (e.g. scaling, normalization)\n",
    "* Create an instance of the SVC classifier and train it on the training data\n",
    "* hse the trained classifier to predict the labels of the testing datW\n",
    "* Evaluate the performance of the classifier using any metric of your choice (e.g. accuracy,\n",
    "precision, recall, F1-scoreK\n",
    "* Tune the hyperparameters of the SVC classifier using GridSearchCV or RandomizedSearchCV to\n",
    "improve its performanc_\n",
    "* Train the tuned classifier on the entire dataset\n",
    "* Save the trained classifier to a file for future use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2237c40d-8094-4ef5-bb4a-3a40894e83a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import joblib\n",
    "X, y = make_classification(n_samples=1000, n_features=10, random_state=42)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "svc_classifier = SVC()\n",
    "svc_classifier.fit(X_train_scaled, y_train)\n",
    "y_pred = svc_classifier.predict(X_test_scaled)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n",
    "param_grid = {\n",
    "    'C': [0.1, 1, 10],\n",
    "    'kernel': ['linear', 'rbf'],\n",
    "    'gamma': ['scale', 'auto']\n",
    "}\n",
    "grid_search = GridSearchCV(estimator=SVC(), param_grid=param_grid, scoring='accuracy', cv=3)\n",
    "grid_search.fit(X_train_scaled, y_train)\n",
    "print(\"Best Parameters:\", grid_search.best_params_)\n",
    "print(\"Best Accuracy:\", grid_search.best_score_)\n",
    "tuned_svc_classifier = grid_search.best_estimator_\n",
    "tuned_svc_classifier.fit(X_scaled, y)\n",
    "joblib.dump(tuned_svc_classifier, 'tuned_svc_classifier.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c32fb3fe-1211-4e2f-afe8-7b7daf279f57",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
