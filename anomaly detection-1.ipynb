{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4d50af5f-335f-475a-a982-c7d78bd631f7",
   "metadata": {},
   "source": [
    "# Question.1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73a4904b-d1a2-4e3e-ba0b-f13d6f958782",
   "metadata": {},
   "source": [
    "## What is anomaly detection and what is its purpose?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "195b2adc-50c4-479f-b58f-b38257dec08c",
   "metadata": {},
   "source": [
    "Anomaly detection refers to the process of identifying patterns or data points that deviate significantly from the norm or expected behavior within a dataset. These anomalies, also known as outliers or novelties, represent data points that are different from the majority of the data in terms of their characteristics, behaviors, or statistical properties.\n",
    "The purpose of anomaly detection is to identify these unusual or unexpected observations, which could indicate various types of events or issues such as:\n",
    "1. **Fraud Detection**: Anomaly detection can be used in financial systems to identify fraudulent transactions, such as unauthorized credit card charges or suspicious account activities.\n",
    "2. **Network Security**: It can be employed to detect unusual patterns in network traffic that might indicate cyber attacks, intrusions, or unauthorized access.\n",
    "3. **Industrial Equipment Monitoring**: Anomaly detection can help identify machinery or equipment failures in industrial settings by monitoring sensor data and detecting deviations from normal operating conditions.\n",
    "4. **Healthcare**: In medical applications, anomaly detection can help in diagnosing diseases by identifying unusual patterns in patient data or medical images.\n",
    "5. **Quality Control**: Anomaly detection is used in manufacturing to identify defective products or processes by detecting deviations from expected standards.\n",
    "6. **Environmental Monitoring**: It can be used to identify unusual environmental events, such as pollution spikes or natural disasters, by detecting anomalies in sensor data.\n",
    "7. **Predictive Maintenance**: Anomaly detection can predict equipment failures or breakdowns by identifying deviations from normal behavior in machinery or equipment sensor data.\n",
    "8. **E-commerce**: It can be used to identify abnormal customer behavior, such as unusual purchasing patterns, that might indicate fraudulent activities.\n",
    "9. **Telecommunications**: Anomaly detection can help in identifying unusual call patterns that might indicate network abuse or fraud.\n",
    "10. **Transportation**: Anomaly detection can help identify unusual patterns in transportation systems, such as sudden increases in traffic congestion or irregularities in public transportation schedules."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa6a84bd-80c9-4aca-abdd-a7fbb36efbec",
   "metadata": {},
   "source": [
    "# Question.2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ec78096-de7c-4d0b-a747-839607b6e66c",
   "metadata": {},
   "source": [
    "## What are the key challenges in anomaly detection?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a82b7cf-6eeb-43b4-8e19-a1ca3bec118b",
   "metadata": {},
   "source": [
    "Anomaly detection is a complex task that involves several challenges, which can vary based on the specific domain and application. Some of the key challenges in anomaly detection include:\n",
    "1. **Imbalanced Data**: In many real-world scenarios, anomalies are rare compared to normal data. This can lead to imbalanced datasets where the number of normal instances far outweighs the number of anomalies. Traditional machine learning algorithms might struggle to detect anomalies effectively in such cases.\n",
    "2. **Unlabeled Data**: Anomaly detection often requires labeled data for training, where anomalies are explicitly marked. However, obtaining labeled data for anomalies can be difficult and expensive, as anomalies are, by definition, uncommon and unexpected.\n",
    "3. **Changing Patterns**: The characteristics of anomalies and normal data can change over time, making it challenging to maintain accurate anomaly detection models. Anomaly detection systems need to adapt to these changes to avoid false negatives or false positives.\n",
    "4. **Noise and Outliers**: Noise in the data and outliers that are not actual anomalies can complicate the detection process. Distinguishing between genuine anomalies and noisy or outlier data points is a common challenge.\n",
    "5. **Feature Engineering**: Selecting and engineering relevant features that capture the underlying patterns of normal and anomalous behavior can be complex, especially when dealing with high-dimensional or complex data.\n",
    "6. **Model Complexity**: Balancing the complexity of anomaly detection models is important. Overly complex models might fit noise in the data, leading to poor generalization. On the other hand, overly simplistic models might not capture the nuances of anomalies.\n",
    "7. **Scalability**: In applications involving large datasets, real-time monitoring, or high-frequency data streams, scalability becomes a challenge. Anomaly detection methods need to be efficient enough to process and analyze data in a timely manner.\n",
    "8. **Contextual Anomalies**: Some anomalies are only anomalies in specific contexts. For example, a sudden increase in web traffic might be normal during a marketing campaign. Anomaly detection systems need to consider contextual information to avoid false alarms.\n",
    "9. **Evaluation Metrics**: Traditional evaluation metrics like accuracy might not be suitable for imbalanced datasets. Specialized metrics, like precision-recall curves or F1-score, are often used to evaluate anomaly detection performance.\n",
    "10. **Human-in-the-Loop**: Anomaly detection systems sometimes require human validation, especially when anomalies are nuanced or context-dependent. Incorporating human feedback into the detection process can be challenging.\n",
    "11. **Concept Drift**: When the underlying distribution of data changes over time (concept drift), models trained on historical data might become less effective at detecting anomalies in the current data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f41fd6e4-4d51-48e9-b747-7910358e8473",
   "metadata": {},
   "source": [
    "# Question.3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8da5fa82-d105-4ab2-bf98-93896e6817f0",
   "metadata": {},
   "source": [
    "## How does unsupervised anomaly detection differ from supervised anomaly detection?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e940228b-a2eb-4c31-b138-bc9578771b08",
   "metadata": {},
   "source": [
    "Unsupervised anomaly detection and supervised anomaly detection are two different approaches to identifying anomalies in a dataset, each with its own characteristics and use cases. Here's how they differ:\n",
    "\n",
    "1. **Data Labeling**:\n",
    "   - **Unsupervised Anomaly Detection**: In unsupervised anomaly detection, you have a dataset that contains only normal data points, without any explicit labels indicating which instances are anomalies. The goal is to identify data points that deviate significantly from the normal behavior without using labeled anomalies for training.\n",
    "   - **Supervised Anomaly Detection**: In supervised anomaly detection, you have a labeled dataset that includes both normal data points and annotated anomalies. The model learns from this labeled data to distinguish between normal and anomalous instances.\n",
    "2. **Training Data**:\n",
    "   - **Unsupervised Anomaly Detection**: Unsupervised methods rely solely on the characteristics of normal data to identify anomalies. They aim to learn the underlying distribution of the normal data and then identify deviations from this learned pattern.\n",
    "   - **Supervised Anomaly Detection**: Supervised methods utilize both normal and anomalous instances during training. They learn the boundaries that separate normal and anomalous data points based on the labeled examples.\n",
    "3. **Modeling Approach**:\n",
    "   - **Unsupervised Anomaly Detection**: Unsupervised methods often use techniques like clustering, density estimation, or distribution fitting to identify regions of low-density data. Points that fall into these low-density regions are considered anomalies.\n",
    "   - **Supervised Anomaly Detection**: Supervised methods typically involve training a classifier (e.g., decision trees, support vector machines, neural networks) on the labeled dataset. The classifier learns to differentiate between normal and anomalous instances based on the provided labels.\n",
    "4. **Data Availability**:\n",
    "   - **Unsupervised Anomaly Detection**: Unsupervised methods are particularly useful when labeled anomalies are scarce or expensive to obtain. They can be applied in scenarios where it's difficult to collect a representative set of anomalies for training.\n",
    "   - **Supervised Anomaly Detection**: Supervised methods require labeled anomaly data for training. They are suitable when a sufficient amount of labeled anomalies is available and when you want a model that directly predicts anomaly labels.\n",
    "5. **Adaptability to New Anomalies**:\n",
    "   - **Unsupervised Anomaly Detection**: Unsupervised methods can potentially detect novel or previously unseen anomalies, as they focus on deviations from normal behavior. However, they might also be more prone to false positives if the normal data distribution changes.\n",
    "   - **Supervised Anomaly Detection**: Supervised methods are better suited for detecting anomalies that are similar to the labeled anomalies used during training. They might struggle to identify novel anomalies that significantly differ from the training set."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b12346e0-5666-48d9-8ea0-f5ae208e91c9",
   "metadata": {},
   "source": [
    "# Question.4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d6efe67-ac1e-469a-b5bb-95e4fb3f8139",
   "metadata": {},
   "source": [
    "## What are the main categories of anomaly detection algorithms?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d46ae6ee-5d42-4402-af88-9871162aa225",
   "metadata": {},
   "source": [
    "Anomaly detection algorithms can be broadly categorized into several main groups based on their underlying principles and techniques. These categories include:\n",
    "\n",
    "1. **Statistical Methods**:\n",
    "   - **Z-Score / Standard Deviation**: Identifying anomalies based on how many standard deviations a data point is away from the mean.\n",
    "   - **Percentiles / Quartiles**: Detecting anomalies by comparing data points against percentiles or quartiles of the data distribution.\n",
    "   - **Histogram-based Methods**: Creating histograms of data and identifying anomalies as points that fall in low-frequency bins.\n",
    "   - **Probability Density Estimation**: Modeling the distribution of normal data and flagging data points with low probability density as anomalies.\n",
    "2. **Machine Learning Algorithms**:\n",
    "   - **Clustering Algorithms**: Identifying anomalies as data points that don't belong to any cluster or are in sparsely populated clusters.\n",
    "   - **Classification Algorithms**: Training a classifier to distinguish between normal and anomalous instances based on labeled data.\n",
    "   - **Isolation Forest**: Constructing decision trees that isolate anomalies faster than normal instances.\n",
    "   - **One-Class SVM (Support Vector Machine)**: Training a model to create a boundary around normal data points and identifying points outside this boundary as anomalies.\n",
    "   - **Autoencoders**: Neural networks trained to reconstruct input data; anomalies result in poor reconstructions.\n",
    "   - **K-Nearest Neighbors (KNN)**: Flagging instances that have few neighbors or are distant from most neighbors as anomalies.\n",
    "3. **Distance-Based Methods**:\n",
    "   - **Mahalanobis Distance**: Measures the distance of a data point from the center of the data distribution, considering covariance.\n",
    "   - **Euclidean Distance**: Detecting anomalies based on how far a data point is from its nearest neighbors.\n",
    "4. **Density-Based Methods**:\n",
    "   - **DBSCAN (Density-Based Spatial Clustering of Applications with Noise)**: Identifying anomalies as noise points or points in sparse areas of the data space.\n",
    "   - **LOF (Local Outlier Factor)**: Identifying data points with significantly lower density compared to their neighbors.\n",
    "5. **Time-Series Methods**:\n",
    "   - **ARIMA (AutoRegressive Integrated Moving Average)**: Forecasting future values and identifying anomalies as data points that deviate significantly from the forecast.\n",
    "   - **Exponential Smoothing**: Similar to ARIMA, this method forecasts based on weighted averages of past observations.\n",
    "6. **Ensemble Methods**:\n",
    "   - **Combining Multiple Algorithms**: Creating an ensemble of different anomaly detection methods to improve overall performance.\n",
    "7. **Deep Learning Methods**:\n",
    "   - **Variational Autoencoders (VAE)**: A type of autoencoder that models data distribution and identifies anomalies as data points with low probability in this distribution.\n",
    "   - **Generative Adversarial Networks (GANs)**: Generating data samples and identifying anomalies as instances that are difficult to generate.\n",
    "8. **Domain-Specific Methods**:\n",
    "   - Some domains might require specialized techniques tailored to the unique characteristics of the data, such as audio, image, or text data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54c11445-bd71-43fb-b0d8-1786d91d4a3b",
   "metadata": {},
   "source": [
    "# Question.5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e07a8079-e290-4bd2-a452-c2ba908281c7",
   "metadata": {},
   "source": [
    "## What are the main assumptions made by distance-based anomaly detection methods?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0c9aad6-8edd-42a4-81a8-3da1a41809ff",
   "metadata": {},
   "source": [
    "Distance-based anomaly detection methods make certain assumptions about the distribution and characteristics of data in order to identify anomalies based on distances between data points. Some of the main assumptions made by these methods include:\n",
    "\n",
    "1. **Normal Data Distribution**: Distance-based methods often assume that the majority of the data points are generated from a normal or Gaussian distribution. This assumption allows them to define a central region where most normal data points lie.\n",
    "2. **Compactness of Normal Data**: It's assumed that normal data points are relatively densely clustered or compact within the feature space. Anomalies, being different from the norm, are expected to be located in less dense regions.\n",
    "3. **Distance to Neighbors**: The distance between a data point and its neighbors is crucial. It's assumed that normal data points have similar neighbors within a certain distance, while anomalies might have few or distant neighbors.\n",
    "4. **Threshold-Based Detection**: Many distance-based methods use a threshold distance or a certain percentile of distances to define a boundary between normal and anomalous data. Points beyond this boundary are considered anomalies.\n",
    "5. **Global vs. Local Behavior**: Some distance-based methods might assume that anomalies can be detected based on global or overall data distribution characteristics, while others might focus on local regions where anomalies exhibit more noticeable deviations.\n",
    "6. **Symmetry of Data Distribution**: In cases where distance metrics are used, like Euclidean distance or Mahalanobis distance, these methods often assume that the data distribution is symmetric and that the distances between points are meaningful.\n",
    "7. **Homogeneous Data**: Assumptions are often based on the data being homogeneous, meaning that the underlying data distribution remains relatively consistent throughout the dataset.\n",
    "8. **Limited Outliers**: These methods typically assume that anomalies are relatively rare in the dataset compared to normal instances, which aligns with the real-world occurrence of anomalies."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dccb5c41-3a05-4efc-8568-21446a9028be",
   "metadata": {},
   "source": [
    "# Question.6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ea8b2bc-2aa2-41af-9648-cda46559a632",
   "metadata": {},
   "source": [
    "## How does the LOF algorithm compute anomaly scores?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bf22264-2d3a-4d48-b863-889634b22922",
   "metadata": {},
   "source": [
    "The LOF (Local Outlier Factor) algorithm computes anomaly scores to identify anomalies in a dataset based on their local density compared to the densities of their neighboring data points. The LOF algorithm aims to capture the relative density deviation of each data point with respect to its neighbors. Here's how the algorithm computes anomaly scores:\n",
    "\n",
    "1. **Local Reachability Density (LRD) Calculation**:\n",
    "   For each data point \\(p\\), LOF calculates its local reachability density (LRD). The LRD of a point \\(p\\) measures how densely the neighborhood of \\(p\\) is populated. It's defined as the inverse of the average reachability distance of \\(p\\) to its \\(k\\) nearest neighbors, where \\(k\\) is a user-defined parameter (usually small).\n",
    "   The reachability distance from point \\(p\\) to point \\(q\\) is the maximum of the distance between \\(p\\) and \\(q\\) and the \\(k\\)-distance of \\(q\\) (distance to its \\(k\\)th nearest neighbor). This measures how \"reachable\" a point is from its neighbors.\n",
    "2. **Local Outlier Factor (LOF) Calculation**: \n",
    "   Once the LRD values are computed, the LOF for each data point \\(p\\) is calculated. The LOF of \\(p\\) quantifie how much the local density of \\(p\\) deviates from the densities of its neighbors. It's calculated as the ratio of the average LRD of the neighbors of \\(p\\) to the LRD of \\(p\\):\n",
    "   \\[\\text{LOF}_p = \\frac{\\sum_{o \\in N_p} \\text{LRD}_o}{\\text{LRD}_p \\times |N_p|},\\]\n",
    "   where \\(N_p\\) is the set of neighbors of \\(p\\), and \\(|N_p|\\) is the number of neighbors.\n",
    "3. **Anomaly Score**:\n",
    "   The anomaly score of each point \\(p\\) is simply its computed LOF value. Higher LOF values indicate that the data point \\(p\\) is less dense compared to its neighbors and, therefore, more likely to be an anomaly. Lower LOF values correspond to points that are denser than their neighbors and are considered more normal.\n",
    "4. **Interpreting Anomaly Scores**:\n",
    "   Anomaly scores close to 1 indicate that a point's density is similar to its neighbors, while scores significantly greater than 1 indicate that a point's density is much lower than its neighbors, suggesting it's an anomaly. It's important to set a threshold for anomaly detection; points with LOF values above this threshold are flagged as anomalies."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5328818c-4ad4-4ae4-8170-8fe626472504",
   "metadata": {},
   "source": [
    "# Question.7"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7dc19da-50bf-4b0d-bb3c-f43a43304ec4",
   "metadata": {},
   "source": [
    "## What are the key parameters of the Isolation Forest algorithm?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d2d36a8-3daf-4224-9b36-e510a3b6d88b",
   "metadata": {},
   "source": [
    "The Isolation Forest algorithm is an anomaly detection technique that uses an ensemble of isolation trees to identify anomalies in a dataset. It operates by isolating anomalies into smaller partitions (trees) in fewer steps compared to normal instances. Here are the key parameters of the Isolation Forest algorithm:\n",
    "1. **n_estimators**:\n",
    "   - This parameter determines the number of isolation trees to create in the ensemble. Increasing the number of trees can lead to a more accurate anomaly detection but also increases computational complexity.\n",
    "2. **max_samples**:\n",
    "   - It controls the number of samples used to build each isolation tree. It can be set as a fraction of the total number of data points or an integer representing the exact number of samples to use.\n",
    "3. **contamination**:\n",
    "   - This parameter specifies the expected proportion of anomalies in the dataset. It helps the algorithm estimate the threshold for considering a point as an anomaly. The default value is set to \"auto,\" which estimates the contamination based on the dataset.\n",
    "4. **max_features**:\n",
    "   - It determines the maximum number of features to consider when splitting a node in an isolation tree. This parameter can be set as an integer or a fraction of the total number of features. A lower value can lead to faster training but might also reduce the ability of trees to separate anomalies.\n",
    "5. **bootstrap**:\n",
    "   - If set to True, each isolation tree will be built using bootstrap sampling (sampling with replacement). If set to False, the entire dataset will be used for building each tree.\n",
    "6. **random_state**:\n",
    "   - This parameter is used to seed the random number generator for reproducibility. Providing a specific value ensures that the same random sequence is used across different runs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1238bc0-170b-40cf-9e0b-23f34b501052",
   "metadata": {},
   "source": [
    "# Question.8"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce51f749-f551-43ab-9a47-20f1a6132238",
   "metadata": {},
   "source": [
    "## If a data point has only 2 neighbours of the same class within a radius of 0.5, what is its anomaly score using KNN with K=10?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eee0a774-2c47-483d-b007-283a1a787403",
   "metadata": {},
   "source": [
    "In the context of KNN (K-Nearest Neighbors) anomaly detection, the anomaly score of a data point is typically based on the distance to its k-nearest neighbors, where k is a user-defined parameter. If a data point has only 2 neighbors of the same class within a radius of 0.5, and you're using KNN with k=10, then it's important to clarify a few things:\n",
    "\n",
    "1. **Distance Metric**: In KNN, the choice of distance metric matters. Common choices include Euclidean distance, Manhattan distance, etc. The distance metric you use will impact how the neighbors are selected based on the radius.\n",
    "\n",
    "2. **Density-Based vs. KNN Approach**: The scenario you're describing sounds more like a density-based approach (counting neighbors within a certain radius) rather than the typical KNN approach (considering a fixed number of nearest neighbors). In a density-based approach, a point's anomaly score could be calculated based on the relative sparsity of its neighborhood.\n",
    "\n",
    "Given that you're using KNN with k=10 and describing a density-based scenario, here's a general idea of how you might calculate the anomaly score:\n",
    "\n",
    "1. Find the k-nearest neighbors of the data point using the specified distance metric (Euclidean, Manhattan, etc.). In this case, k=10.\n",
    "\n",
    "2. Check how many of these k-nearest neighbors are within the radius of 0.5. If only 2 neighbors are within this radius and they are of the same class, this suggests that the local density around the data point is low, and it might be considered an anomaly.\n",
    "\n",
    "3. Compute the anomaly score based on the density or sparsity of the neighborhood. For example, you could use a simple formula like (2 / 10), which represents the fraction of neighbors within the radius.\n",
    "\n",
    "   Anomaly Score = (Number of Neighbors within Radius) / k"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6e81655-8291-4c84-aaba-7f4a57784abc",
   "metadata": {},
   "source": [
    "# Question.9"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1fd3859-a250-41bb-9a87-1a84d85f48d8",
   "metadata": {},
   "source": [
    "## Using the Isolation Forest algorithm with 100 trees and a dataset of 3000 data points, what is the anomaly score for a data point that has an average path length of 5.0 compared to the average path length of the trees?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0a38f40-c259-465d-8081-97131b23b8c5",
   "metadata": {},
   "source": [
    "The Isolation Forest algorithm assigns anomaly scores to data points based on their average path length in the isolation trees that make up the ensemble. The average path length reflects how quickly a data point is isolated within the trees. Anomalies are expected to have shorter average path lengths compared to normal instances.\n",
    "\n",
    "The formula to compute the anomaly score for a data point based on its average path length \\( \\text{APL}(x) \\) and the average path length of the trees \\( \\text{APL}_{\\text{avg}} \\) is as follows:\n",
    "\n",
    "\\[ \\text{Anomaly Score}(x) = 2^{-\\frac{\\text{APL}(x)}{\\text{APL}_{\\text{avg}}}}. \\]\n",
    "\n",
    "Given your information:\n",
    "\n",
    "- Number of trees (\\( \\text{n\\_estimators} \\)): 100\n",
    "- Dataset size (\\( \\text{dataset size} \\)): 3000\n",
    "- Average path length of the data point (\\( \\text{APL}(x) \\)): 5.0\n",
    "\n",
    "Let's assume that the average path length of the trees (\\( \\text{APL}_{\\text{avg}} \\)) is not provided and needs to be calculated. The average path length of the trees is typically calculated based on the depth of the trees in the ensemble. The deeper the trees, the higher the average path length. However, the exact value of \\( \\text{APL}_{\\text{avg}} \\) depends on the specific structure of the trees in the forest.\n",
    "\n",
    "Assuming you have the value of \\( \\text{APL}_{\\text{avg}} \\), you can plug in the values into the formula to calculate the anomaly score for the given data point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e81ca87-b641-4bd9-8ea5-278a723c352e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ba7fe1a-1c85-47d9-86ab-797f56970cea",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
